
\documentclass{article}
\usepackage[utf8]{inputenc} % Allow utf-8 input
\usepackage[T1]{fontenc}    % Use 8-bit T1 fonts
\usepackage{graphicx}       % Required for inserting images
\usepackage{booktabs}       % Professional-quality tables
\usepackage{amsfonts}       % Blackboard math symbols
\usepackage{nicefrac}       % Compact symbols for 1/2, etc.
\usepackage{microtype}      % Microtypography
\usepackage{blkarray}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=red, filecolor=magenta, urlcolor=cyan]{hyperref} % Hyperlinks
\usepackage{halloweenmath}
\usepackage{subcaption}
\usepackage{calrsfs}
\usepackage{listings}
\usepackage{authblk}
\usepackage{ragged2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[dvipsnames]{xcolor}
\usepackage{geometry}
\usepackage{csquotes}


% Define colors for Python syntax highlighting
\definecolor{pythonblue}{RGB}{0,0,255}
\definecolor{pythongreen}{RGB}{0,128,0}
\definecolor{pythonpurple}{RGB}{128,0,128}
\definecolor{pythongray}{RGB}{128,128,128}

% Set up Python code style
\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{pythonblue},
    stringstyle=\color{pythongreen},
    commentstyle=\color{pythongray},
    emph={import,from,class,def,for,while,if,is,in,elif,else,return,try,except,finally},
    emphstyle=\color{pythonpurple},
    framesep=5pt,
    breaklines=true,
    showstringspaces=false,
    tabsize=4
}

\geometry{a4paper, top=0.5in, bottom=1in, left=1in, right=1in}

\makeatletter
\renewcommand{\maketitle}{
    \begin{center}
        \vspace{-2.3em} % Move the title closer to the top
        {\LARGE \textbf{\@title} \par}
        \vspace{1em} % Adjust spacing between title and author
        {\@author \par}
        \vspace{-1em} % Reduce spacing before content
        \@date
    \end{center}
}
\makeatother


\usepackage{setspace}
\setstretch{0.9}  % Adjust the number to reduce line spacing (1.0 is default)

\setlength{\parskip}{1pt}

\begin{document}
\title{\textbf{Clustering Spotify Podcasts with NLP-Driven Insights}}
\author{}
\date{}

\maketitle
\pagenumbering{arabic} % Start normal page numbering

\noindent \textbf{Data Collection, Cleaning, and Tokenization:} 

\noindent Using Selenium and Spotify API, we scraped the top 50 podcasts per genre (\href{https://github.com/Stochastic1017/Spotify-Podcast-Clustering/blob/main/spotify_api/fetch_top_podcast.py}{\texttt{fetch\_top\_podcast.py}}). Metadata was retrieved and filtered for English podcasts (\href{https://github.com/Stochastic1017/Spotify-Podcast-Clustering/blob/main/spotify_api/fetch_podcast_details.py}{\texttt{fetch\_podcast\_details.py}}), followed by episode details (\href{https://github.com/Stochastic1017/Spotify-Podcast-Clustering/blob/main/spotify_api/fetch_episode_details.py}{\texttt{fetch\_episode\_details.py}}), yielding 284,481 episodes. Episode descriptions were cleaned and tokenized (\href{https://github.com/Stochastic1017/Spotify-Podcast-Clustering/blob/main/tokenization/clean_description.py}{\texttt{clean\_description.py}}), performing normalization, URL removal, lemmatization, and stopword removal. Tokens for each podcast were consolidated into "frequency" vectors relative to a global vocabulary of 47,718 tokens (total number of unique tokens for all podcasts).\\

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Related Tokens} & \textbf{Frequency} & \textbf{Unrelated Tokens} & \textbf{Frequency} \\ \hline
        murder       & 47       & Technology & 1 \\ \hline
        crime        & 33        & Sleep      & 0 \\ \hline
        killers      & 21        & Comedy     & 0 \\ \hline
        cover        & 12        & Finance    & 0 \\ \hline
        mysterious   & 5        & Cooking    & 0 \\ \hline
        survival     & 3        & Science    & 1 \\ \hline
    \end{tabular}
    \caption{Frequency vector for a true crime podcast, showing high frequencies for related tokens and very low or zero frequencies for unrelated tokens.}
    \label{tab:frequency_vector}
\end{table}



\noindent \textbf{Computing metrics:} 

\noindent Three metrics were computed (\href{https://github.com/Stochastic1017/Spotify-Podcast-Clustering/blob/main/models/compute_metrics.py}{\texttt{compute\_metrics.py}}) using frequency vectors $\bm{x}$ and $\bm{y}$ (both of length 47,718) for any two podcasts as follows:

\begin{enumerate}
    \item \textit{Normalized Total Feature Similarity:} Measures cosine similarity between two frequency vectors.
    $$\text{NTFS}(\bm{x},\bm{y}) = \frac{\langle \bm{x}, \bm{y}\rangle}{||\bm{x}||_{2}\;||\bm{y}||_{2}} \in \mathbb{R}_{[0,1]} \quad \longrightarrow \quad \text{higher implies more directional similarity}$$
    Strengths: Robust for sparse vectors. Weakness: Assumes all tokens equally important.
    \item \textit{Jaccard Token Similarity:} Compute metric signifying proportion of overlapping tokens.
    $$\text{JTS}(\bm{x},\bm{y}) = \frac{\sum \text{min}(x_i, y_i)}{\sum \text{max}(x_i, y_i)} \in \mathbb{R}_{[0,1]} \quad \longrightarrow \quad \text{higher implies more token overlap}$$
    Strengths: Simple  and interpretable measure of overlap. Weakness: Sensitive to scaling.
    \item \textit{Weighted Token Diversity Similarity:} Uses L1-normalized frequency vectors that emphasizing token diversity.
    $$ \text{WTDS}(\bm{x},\bm{y}) = \sum_{i=1}^{n} \sqrt{ \frac{x_i}{||\bm{x}||_{1}} \cdot \frac{y_i}{||\bm{y}||_{1}} } \in \mathbb{R}_{[0,1]} \quad \longrightarrow \quad \text{higher implies more shared diversity}$$
    Strength: Highlights diversity. Weakness: Assumes uniform importance across tokens.
\end{enumerate}


\noindent \textbf{Recommendation algorithm:} 

\noindent Given a selected podcast $k$, generate $n$-recommendations from a list of $T$ podcasts as follows (\href{https://github.com/Stochastic1017/Spotify-Podcast-Clustering/blob/main/dash_app/helpers/scatterplot.py}{scatterplot.py}). We construct a vector of 3-dimensional tuples of similarity metrics, for all $i=1,\dots,T$.
\[
\begin{blockarray}{cccccc}
    & \text{podcast 1} & \dots & \text{podcast k} & \dots & \text{podcast T} \\
    \begin{block}{c(ccccc)}
    \text{podcast k} & \mathcal{S}_{k,1} & \dots & \mathcal{S}_{k,k} & \dots & \mathcal{S}_{k,T} \\
    \end{block}
\end{blockarray}
\]

\noindent where, $\mathcal{S}_{k,i} = \begin{cases}
\big( \; \text{NTFS}(\bm{x_k}, \bm{x_i}), \; \text{JTS}(\bm{x_k}, \bm{x_i}), \; \text{WTDS}(\bm{x_k}, \bm{x_i}) \;\big) & \text{if}\; i \neq k\\
\\
\big(1,1,1)                                                          & \text{if}\; i = k
\end{cases}$\\

\noindent Next, we quantify dissimilarity by computing the euclidean 2-norm distance with respect to podcast $k$:
$$d_{ki} = ||\underbrace{(1,1,1)}_{\mathcal{S}_{k,k}} - \mathcal{S}_{k,i}||_2 = \sqrt{\big(1 - \text{NTFS}(\bm{x_k}, \bm{x_i})\big)^2 + \big(1 - \text{JTS}(\bm{x_k}, \bm{x_i})\big)^2 + \big(1 - \text{WTDS}(\bm{x_k}, \bm{x_i})\big)^2}$$

\noindent Finally, we sort by distance (lowest to highest) and report the $n$-closest podcasts. Each reported podcast represents those whose description match most closely in direction, shared content coverage, and diversity of content to podcast $k$, ensuring tailored recommendations for enhancing user engagement.


\end{document}
